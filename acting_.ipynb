{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/seungeunrho/minimalRL/blob/master/ddpg.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import collections\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.set_default_tensor_type('torch.DoubleTensor')\n",
    "\n",
    "#Hyperparameters\n",
    "lr_mu        = 0.0001\n",
    "lr_q         = 0.0001\n",
    "gamma        = 0.99\n",
    "batch_size   = 128\n",
    "buffer_limit = 100000\n",
    "tau          = 0.001 # for target network soft update\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self):\n",
    "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
    "\n",
    "    def put(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "    \n",
    "    def sample(self, n):\n",
    "        mini_batch = random.sample(self.buffer, n)\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
    "\n",
    "        for transition in mini_batch:\n",
    "            s, a, r, s_prime, done_mask = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            done_mask_lst.append([done_mask])\n",
    "        \n",
    "        return torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n",
    "               torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), \\\n",
    "               torch.tensor(done_mask_lst)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class MuNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MuNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(33, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc_mu = nn.Linear(128, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x.double()))\n",
    "        x = F.relu(self.fc2(x.double()))\n",
    "        mu = torch.tanh(self.fc_mu(x))\n",
    "        return mu\n",
    "\n",
    "class QNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QNet, self).__init__()\n",
    "        \n",
    "        self.fc_s = nn.Linear(33, 256)\n",
    "        #self.fc_a = nn.Linear(4,  28)\n",
    "        self.fc_q = nn.Linear(260, 128)\n",
    "        self.fc_3 = nn.Linear(128,  1)\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        h1 = self.fc_s(x.double())\n",
    "        h1 = F.relu(h1)\n",
    "        \n",
    "        a = torch.squeeze(a.double())\n",
    "        #h2 = self.fc_a(a)\n",
    "        #h2 = F.relu(h2)\n",
    "\n",
    "        cat = torch.cat([h1, a], dim=1)\n",
    "        q = F.relu(self.fc_q(cat))\n",
    "        q = self.fc_3(q)\n",
    "        return q\n",
    "\n",
    "class OrnsteinUhlenbeckNoise:\n",
    "    def __init__(self, mu):\n",
    "        self.theta, self.dt, self.sigma = 0., 0.0, 0.\n",
    "        self.mu = mu\n",
    "        self.x_prev = np.zeros_like(self.mu)\n",
    "\n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
    "                self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "      \n",
    "def train(mu, mu_target, q, q_target, memory, q_optimizer, mu_optimizer):\n",
    "    s,a,r,s_prime,done_mask  = memory.sample(batch_size)\n",
    "    \n",
    "    target = r + gamma * q_target(s_prime, mu_target(s_prime))\n",
    "    q_loss = F.smooth_l1_loss(q(s,a), target.detach())\n",
    "    q_optimizer.zero_grad()\n",
    "    q_loss.backward()\n",
    "    q_optimizer.step()\n",
    "    \n",
    "    mu_loss = -q(s,mu(s)).mean() # That's all for the policy loss.\n",
    "    mu_optimizer.zero_grad()\n",
    "    mu_loss.backward()\n",
    "    mu_optimizer.step()\n",
    "    \n",
    "def soft_update(net, net_target):\n",
    "    for param_target, param in zip(net_target.parameters(), net.parameters()):\n",
    "        param_target.data.copy_(param_target.data * (1.0 - tau) + param.data * tau)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "INFO:root:\n",
      "Unfortunately, your original traceback can not be constructed.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/pablot/anaconda3/envs/drlnd/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-3-125d10316b1b>\", line 34, in <module>\n",
      "    env_info = env.step(a)[brain_name]\n",
      "  File \"/Users/pablot/anaconda3/envs/drlnd/lib/python3.6/site-packages/unityagents/environment.py\", line 369, in step\n",
      "    self._generate_step_input(vector_action, memory, text_action)\n",
      "  File \"/Users/pablot/anaconda3/envs/drlnd/lib/python3.6/site-packages/unityagents/rpc_communicator.py\", line 78, in exchange\n",
      "    output = self.unity_to_external.parent_conn.recv()\n",
      "  File \"/Users/pablot/anaconda3/envs/drlnd/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/Users/pablot/anaconda3/envs/drlnd/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/pablot/anaconda3/envs/drlnd/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/pablot/anaconda3/envs/drlnd/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/pablot/anaconda3/envs/drlnd/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1148, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Users/pablot/anaconda3/envs/drlnd/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/pablot/anaconda3/envs/drlnd/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/Users/pablot/anaconda3/envs/drlnd/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/Users/pablot/anaconda3/envs/drlnd/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/Users/pablot/anaconda3/envs/drlnd/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/Users/pablot/anaconda3/envs/drlnd/lib/python3.6/inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/Users/pablot/anaconda3/envs/drlnd/lib/python3.6/inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/Users/pablot/anaconda3/envs/drlnd/lib/python3.6/inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/Users/pablot/anaconda3/envs/drlnd/lib/python3.6/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='Reacher.app')\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "memory = ReplayBuffer()\n",
    "\n",
    "q, q_target = QNet(), QNet()\n",
    "q_target.load_state_dict(q.state_dict())\n",
    "mu, mu_target = MuNet(), MuNet()\n",
    "mu_target.load_state_dict(mu.state_dict())\n",
    "\n",
    "mu.load_state_dict(torch.load('mu.pth'))\n",
    "mu_target.load_state_dict(torch.load('mu.pth'))\n",
    "q.load_state_dict(torch.load('q.pth'))\n",
    "q_target.load_state_dict(torch.load('q.pth'))\n",
    "\n",
    "score = 0.0\n",
    "print_interval = 1\n",
    "\n",
    "mu_optimizer = optim.Adam(mu.parameters(), lr=lr_mu)\n",
    "q_optimizer  = optim.Adam(q.parameters(), lr=lr_q)\n",
    "ou_noise = OrnsteinUhlenbeckNoise(mu=np.zeros(4))\n",
    "\n",
    "for n_epi in range(1000):\n",
    "    env_info = env.reset(train_mode=False)[brain_name]\n",
    "    s = env_info.vector_observations\n",
    "\n",
    "    while True: # maximum length of episode is 200 for Pendulum-v0\n",
    "        a = mu(torch.from_numpy(s)) \n",
    "        a = a.detach().numpy() + ou_noise()\n",
    "        \n",
    "        env_info = env.step(a)[brain_name]\n",
    "        s_prime = env_info.vector_observations\n",
    "        r = env_info.rewards[0]\n",
    "        done = env_info.local_done\n",
    "        \n",
    "        #memory.put((s[0], a[0], r,s_prime[0], done))\n",
    "        score += r\n",
    "        s = s_prime\n",
    "        \n",
    "        if False:\n",
    "            train(mu, mu_target, q, q_target, memory, q_optimizer, mu_optimizer)\n",
    "            \n",
    "            soft_update(mu, mu_target)\n",
    "            soft_update(q,  q_target)\n",
    "            \n",
    "        if np.any(done):\n",
    "            #print(f'episode {n_epi}: {score}')\n",
    "            break\n",
    "    \n",
    "    if n_epi%print_interval==0 and n_epi!=0:\n",
    "        print(\"# of episode :{}, avg score : {:.1f}\".format(n_epi, score/print_interval))\n",
    "        score = 0.0\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
