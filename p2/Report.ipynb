{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "\n",
    "This project trains an agent in a Unity environment to move a robotic arm to reach a green ball that is circling around it. There are **4 possible continuous acitions** that apply a torque to the joints to control the final position of the hand along time:\n",
    "    \n",
    "A reward of +0.1 is given if the hand of the robot is close to the objective.\n",
    "\n",
    "The whole state is complex, but the state we observe is made of **33 different signals** that the agent takes as inputs. With that inputs the agent must learn a policy to act to maximize reward.\n",
    "\n",
    "There are 2 versions of the environment. The version 1 consist on just one agent. The version 2 consists on 20 pararell agents, but the problem is the same. A environment is considered solved when:\n",
    "\n",
    " - [version 1] the agent receives an average reward (over 100 episodes) of at least +30, or\n",
    " - [version 2] the agent is able to receive an average reward (over 100 episodes, and over all 20 agents) of at least +30.\n",
    " \n",
    "The problem chosen here was the **version 1**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Algorithm:\n",
    "\n",
    "The implemented algorithm is a DDPG Agent. This algorithm is very similar to the Vanilla Policy Gradient (or REINFORCE), but it has some slight differences. The Actor netowrk creates a deterministic policy (hence the name of the algorithm), and this action(s) are fed into the Critic network as an input, in order to evaluare the quality of that state/action pair. He we can expand the Actor-Critic architecture to the continuous domain.\n",
    "\n",
    "![reward plot](imgs/ddpg.png)\n",
    "\n",
    "source: https://www.researchgate.net/figure/Diagram-of-the-actor-critic-architecture-for-DDPG_fig1_333652544\n",
    "\n",
    "It uses a Neural Network with the following architecture:\n",
    "    \n",
    "**MU NETWORK**\n",
    "\n",
    "    Input shape: (33, )\n",
    "    Dense_1 : 256 neurons, ReLU activation\n",
    "    Dense_2 : 128 neurons, ReLU activation\n",
    "    Output: 4 actions (TanH activation)\n",
    "\n",
    "**MU NETWORK**\n",
    "\n",
    "    Input shape: (33, )\n",
    "    Dense_1 : 256 neurons, ReLU activation\n",
    "    Dense_2 : 128 neurons, ReLU activation [input = previous layer + actions]\n",
    "    Output: 4 actions (TanH activation)\n",
    "\n",
    "\n",
    "A soft weight update of the target Mu and Q Network is made with a blending factor TAU=0.001, every timesteps.\n",
    "\n",
    "To train, a batch size of 128 is used, along with a Replay Buffer of capacity 100000. The optimizer is Adam with a LR of 10e-3 for both networks\n",
    "\n",
    "A discount factor GAMMA of 0.99 is used.\n",
    "\n",
    "\n",
    "The number of episodes is set to 1000. Every episode run until its done.\n",
    "\n",
    "The model weights are saved in '*mu.pth*' and  '*q.pth*'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of reward\n",
    "\n",
    "This is the plot of reward over time. The training is finished when the average reward is over 30 (red line)\n",
    "\n",
    "![reward plot](imgs/rewards.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas for Future Work\n",
    "\n",
    "To improve the performance or speed up training several ideas are proposed:\n",
    "\n",
    "    - Try different NN architectures, this is a problem of hyperparamter tuning.\n",
    "    - Implement Prioritized Experience Replay\n",
    "    - Implement a distributed version\n",
    "    - Experiment with more advanced architectures such as D4PG"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
