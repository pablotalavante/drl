{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Algorithm:\n",
    "\n",
    "The implemented algorithm is a DDPG Agent. This algorithm is very similar to the Vanilla Policy Gradient (or REINFORCE), but it has some slight differences. The Actor netowrk creates a deterministic policy (hence the name of the algorithm), and this action(s) are fed into the Critic network as an input, in order to evaluare the quality of that state/action pair. He we can expand the Actor-Critic architecture to the continuous domain.\n",
    "\n",
    "![reward plot](imgs/ddpg.png)\n",
    "\n",
    "source: https://www.researchgate.net/figure/Diagram-of-the-actor-critic-architecture-for-DDPG_fig1_333652544\n",
    "\n",
    "It uses a Neural Network with the following architecture:\n",
    "    \n",
    "**MU NETWORK**\n",
    "\n",
    "    Input shape: (33, )\n",
    "    Dense_1 : 256 neurons, ReLU activation\n",
    "    Dense_2 : 128 neurons, ReLU activation\n",
    "    Output: 4 actions (TanH activation)\n",
    "\n",
    "**MU NETWORK**\n",
    "\n",
    "    Input shape: (33, )\n",
    "    Dense_1 : 256 neurons, ReLU activation\n",
    "    Dense_2 : 128 neurons, ReLU activation [input = previous layer + actions]\n",
    "    Output: 4 actions (TanH activation)\n",
    "\n",
    "\n",
    "A soft weight update of the target Mu and Q Network is made with a blending factor TAU=0.001, every timesteps.\n",
    "\n",
    "To train, a batch size of 128 is used, along with a Replay Buffer of capacity 100000. The optimizer is Adam with a LR of 10e-3 for both networks\n",
    "\n",
    "A discount factor GAMMA of 0.99 is used.\n",
    "\n",
    "\n",
    "The number of episodes is set to 1000. Every episode run until its done.\n",
    "\n",
    "The model weights are saved in '*mu.pth*' and  '*q.pth*'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of reward\n",
    "\n",
    "This is the plot of reward over time. The training is finished when the average reward is over 30 (red line)\n",
    "\n",
    "![reward plot](imgs/rewards.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas for Future Work\n",
    "\n",
    "To improve the performance or speed up training several ideas are proposed:\n",
    "\n",
    "    - Try different NN architectures, this is a problem of hyperparamter tuning.\n",
    "    - Implement Prioritized Experience Replay\n",
    "    - Implement a distributed version\n",
    "    - Experiment with more advanced architectures such as D4PG"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
