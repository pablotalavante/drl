{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1. If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01. Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Each agent receives its own, local observation. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping.\n",
    "\n",
    "The task is episodic, and in order to solve the environment, your agents must get an **average score of +0.5** (over 100 consecutive episodes, after taking the maximum over both agents). Specifically,\n",
    "\n",
    "After each episode, we add up the rewards that each agent received (without discounting), to get a score for each agent. This yields 2 (potentially different) scores. We then take the maximum of these 2 scores.\n",
    "This yields a single score for each episode.\n",
    "The environment is considered solved, when the average (over 100 episodes) of those scores is at least +0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Algorithm:\n",
    "\n",
    "The implemented algorithm is a Multi Agent DDPG. This DDPG is very similar to the Vanilla Policy Gradient (or REINFORCE), but it has some slight differences. The Actor netowrk creates a deterministic policy (hence the name of the algorithm), and this action(s) are fed into the Critic network as an input, in order to evaluare the quality of that state/action pair. He we can expand the Actor-Critic architecture to the continuous domain.\n",
    "\n",
    "For the multi agent version is slightly more complicated.\n",
    "\n",
    "![reward plot](imgs/model.png)\n",
    "\n",
    "source: https://arxiv.org/pdf/1706.02275.pdf\n",
    "\n",
    "Each Actor just recieves the information of each agent, and outputs its action. This makes possible in the execution phase to take actions just based on it own observations. But to be aware of the rest of the environment, the Critic Network takes the State and the Actions of ALL the agents. This is a source of instability in training, but can lead to a much better performance, since during the training phase the agents can know what other agents are *thinking* and doing, in order to adapt its behaviour to maximize the reward.\n",
    "\n",
    "The Q-Networks for each agent are different, but they both have the same inputs.\n",
    "\n",
    "It uses a Neural Network with the following architecture:\n",
    "    \n",
    "**MU NETWORK**\n",
    "\n",
    "    Input shape: (24, )\n",
    "    Dense_1 : 256 neurons, ReLU activation\n",
    "    Dense_2 : 128 neurons, ReLU activation\n",
    "    Output: 2 actions (TanH activation)\n",
    "\n",
    "**Q NETWORK**\n",
    "\n",
    "    Input shape: (state_size+action_size) * num_agents\n",
    "    Dense_1 : 256 neurons, ReLU activation\n",
    "    Dense_2 : 128 neurons, ReLU activation\n",
    "    Output: 1 value\n",
    "\n",
    "\n",
    "A soft weight update of the target Mu and Q Network is made with a blending factor TAU=0.001, every timesteps.\n",
    "\n",
    "To train, a batch size of 128 is used, along with a Replay Buffer of capacity 100000. The optimizer is Adam with a LR of 10e-3 for both networks\n",
    "\n",
    "A discount factor GAMMA of 0.99 is used.\n",
    "\n",
    "\n",
    "The number of episodes is set to 1000. Every episode run until its done.\n",
    "\n",
    "The model weights are saved in '*mu.pth*' and  '*q.pth*'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of reward\n",
    "\n",
    "This is the plot of reward over time. The training is finished when the average reward is over 0.5 (red line)\n",
    "\n",
    "![reward plot](imgs/scores.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas for Future Work\n",
    "\n",
    "Different hyperparamter \n",
    "\n",
    "To improve the performance or speed up training several ideas are proposed:\n",
    "\n",
    "    - Try a noise schduler. To reduce exploration along the training, we could scale the noise in an epsilon greedy fashion, reducing while the training is in a later stage. This might not work and could prevent from learning more, bute definetly is a good idea to try.\n",
    "    - Implement Prioritized Experience Replay\n",
    "    - Implement a distributed version\n",
    "    - Try Hill Climbing for the Neural Netowrk weights\n",
    "    - Analyze the stability of the training. Since the evironment is non stationary, we coul try freezing one network and train the other. This could be a good exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
